{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkr7VTmL3XjB"
   },
   "source": [
    "# Main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8252,
     "status": "ok",
     "timestamp": 1624550113244,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "gkUiPozb2qyf",
    "outputId": "9c0f6999-df80-4a9f-e382-3653fcd81cfe"
   },
   "outputs": [],
   "source": [
    "#allows to import pretrained generators and discriminators\n",
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For choose the gpu device\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDKg3BXM4oMd"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from os import listdir\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlVDjmQs8UUP"
   },
   "source": [
    "# Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOxIM7sUvseF"
   },
   "source": [
    "**Needed the first time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2idx = {\n",
    "    'adenoma': 0,\n",
    "    'hiperplastic': 1,\n",
    "    'serrated': 2\n",
    "}\n",
    "\n",
    "idx2str = {\n",
    "    0: 'adenoma',\n",
    "    1: 'hiperplastic', \n",
    "    2: 'serrated'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_class(index):\n",
    "    \"\"\"\n",
    "    One-hot encodes a class label represented by an index.\n",
    "\n",
    "    Parameters:\n",
    "    index (int): The index representing the class label that needs to be one-hot encoded.\n",
    "\n",
    "    Returns:\n",
    "    ohe_label (numpy.ndarray): A one-hot encoded array with a length of 2, where the value at the index\n",
    "                               corresponding to the input index is set to 1 and all other values are 0.\n",
    "\n",
    "    Notes:\n",
    "    - This function assumes that there are two classes (binary classification), so the length of the\n",
    "      one-hot encoded array is fixed to 2.\n",
    "    - The function sets the element at the given index to 1, representing the class label, and sets all\n",
    "      other elements to 0, indicating the absence of those classes.\n",
    "    - The input index should be in the range [0, 1], representing the two classes.\n",
    "    \"\"\"\n",
    "    ohe_label = np.zeros(2, dtype=int)  # Create a zero-filled array of length 2\n",
    "    ohe_label[index] = 1  # Set the value at the given index to 1 for one-hot encoding\n",
    "    return ohe_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, size=(256, 256), rgb=False):\n",
    "    \"\"\"\n",
    "    Loads images from a directory into memory and performs optional resizing and color mode conversion.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the directory containing the images.\n",
    "    size (tuple, optional): A tuple representing the target size for resizing the images. Default is (256, 256).\n",
    "    rgb (bool, optional): If True, the images will be loaded in RGB color mode; if False, they will be loaded in grayscale.\n",
    "                          Default is False.\n",
    "\n",
    "    Returns:\n",
    "    data (numpy.ndarray): A NumPy array containing the loaded and preprocessed image data.\n",
    "    labels (list): A list of one-hot encoded labels corresponding to the loaded images.\n",
    "\n",
    "    Notes:\n",
    "    - The function assumes that the images in the directory are all valid image files.\n",
    "    - It uses the tqdm library to show a progress bar while processing the images.\n",
    "    - Images with the class label 'serrated' are skipped and not loaded into memory.\n",
    "    - The function uses the Keras `load_img` and `img_to_array` functions for image loading and conversion.\n",
    "    - The `size` parameter can be used to resize the images to a specific size before loading them into memory.\n",
    "    - The `rgb` parameter determines whether the images are loaded in RGB (True) or grayscale (False) color mode.\n",
    "    \"\"\"\n",
    "    data_list = list()\n",
    "    label_list = list()\n",
    "\n",
    "    # Determine the color mode based on the rgb parameter\n",
    "    if rgb == False:\n",
    "        color_mode = \"grayscale\"\n",
    "    else:\n",
    "        color_mode = \"rgb\"\n",
    "\n",
    "    # Enumerate filenames in the directory, assuming all are images\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        clase = filename.split('_')[0]\n",
    "        if clase != 'serrated':\n",
    "            # Load and resize the image\n",
    "            pixels = load_img(path + filename, target_size=size, color_mode=color_mode)\n",
    "            # Convert to a numpy array\n",
    "            pixels = img_to_array(pixels)\n",
    "            # Store the image data\n",
    "            data_list.append(pixels)\n",
    "\n",
    "            # For labels\n",
    "            clase = filename.split('_')[0]\n",
    "            indx = str2idx[clase]  # Assuming there's a dictionary named str2idx mapping class labels to indices\n",
    "            # Get one-hot encoding from the index\n",
    "            ohe_label = ohe_class(indx)  # Assuming there's a function named ohe_class for one-hot encoding\n",
    "            label_list.append(ohe_label)\n",
    "        else:\n",
    "            # Skip images with the class label 'serrated'\n",
    "            None\n",
    "\n",
    "    # Convert the data list to a numpy array and return the data and label lists\n",
    "    return np.asarray(data_list), label_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7lhgM8-8Gcw"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "experiment = 'fineTunedExperimentName' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1957290,
     "status": "ok",
     "timestamp": 1623337285519,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "onwoXFlU7FER",
    "outputId": "dc084731-20c5-4814-cf5c-ea2477bdc78d"
   },
   "outputs": [],
   "source": [
    "\"\"\"Frames loading.\n",
    "rgb parameter sets as False for work whit grayscale images\n",
    "\"\"\"\n",
    "# dataset path\n",
    "path = '../../../data/binary/fold1/'\n",
    "# load dataset white light\n",
    "train_WL_imgs, train_WL_labels = load_images(path + 'train_WL/', rgb= True)\n",
    "test_WL_imgs, test_WL_labels = load_images(path + 'test_WL/', rgb= True)\n",
    "\n",
    "# load dataset NBI\n",
    "train_NBI_imgs, train_NBI_labels = load_images(path + 'train_NBI/', rgb= True)\n",
    "test_NBI_imgs, test_NBI_labels = load_images(path + 'test_NBI/', rgb= True)\n",
    "\n",
    "print(\"train images WL: \", train_WL_imgs.shape, \" labels: \", len(train_WL_labels))\n",
    "print(\"train images NBI: \", train_NBI_imgs.shape, \" labels: \", len(train_NBI_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W96k6Hxs801h"
   },
   "source": [
    "**Data augmentation techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_J-jiJ38nI2"
   },
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    return cropped_image\n",
    "\n",
    "# scaling the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aXhHl2z9CXX"
   },
   "source": [
    "**Preprocess splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-BB_9-b8xKf"
   },
   "outputs": [],
   "source": [
    "def preprocess_image_train(image):\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image):\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tAoNJ_nCeRW"
   },
   "outputs": [],
   "source": [
    "#conversion de las imageness a array\n",
    "train_WL_array = np.asarray(train_WL_imgs)\n",
    "test_WL_array = np.asarray(test_WL_imgs)\n",
    "train_NBI_array = np.asarray(train_NBI_imgs)\n",
    "test_NBI_array = np.asarray(test_NBI_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7VErx21C5UX"
   },
   "outputs": [],
   "source": [
    "#Crea un dataSet de WL y NBI \n",
    "train_WL_ds = tf.data.Dataset.from_tensor_slices(train_WL_array)\n",
    "train_WL_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_WL_labels, tf.int64)).batch(BATCH_SIZE)\n",
    "\n",
    "train_NBI_ds = tf.data.Dataset.from_tensor_slices(train_NBI_array)\n",
    "train_NBI_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_NBI_labels, tf.int64)).batch(BATCH_SIZE)\n",
    "test_WL_ds = tf.data.Dataset.from_tensor_slices(test_WL_array)\n",
    "test_NBI_ds = tf.data.Dataset.from_tensor_slices(test_NBI_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-9xqJNY9KUL"
   },
   "outputs": [],
   "source": [
    "train_WL_ds = train_WL_ds.map(preprocess_image_train, \n",
    "                              num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "train_NBI_ds = train_NBI_ds.map(preprocess_image_train,\n",
    "                                num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "#Since the datasets are in the same order you can just zip them together to get\n",
    "#a dataset of (image, label) pairs:\n",
    "\n",
    "train_WL_image_label_ds = tf.data.Dataset.zip((train_WL_ds, train_WL_label_ds))\n",
    "train_NBI_image_label_ds = tf.data.Dataset.zip((train_NBI_ds, train_NBI_label_ds))\n",
    "\n",
    "#shuffle zip train data\n",
    "train_WL_image_label_ds = train_WL_image_label_ds.shuffle(buffer_size=len(train_WL_image_label_ds),\n",
    "                                                          reshuffle_each_iteration=False)\n",
    "train_WL_image_label_ds = train_WL_image_label_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_NBI_image_label_ds = train_NBI_image_label_ds.shuffle(buffer_size=len(train_NBI_image_label_ds),\n",
    "                                                          reshuffle_each_iteration=False)\n",
    "train_NBI_image_label_ds = train_NBI_image_label_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "#for test data\n",
    "test_WL_ds = test_WL_ds.map(preprocess_image_test,\n",
    "                            num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "test_NBI_ds = test_NBI_ds.map(preprocess_image_test,\n",
    "                              num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sample_WL, lab_sample_WL  = next(iter(train_WL_image_label_ds))\n",
    "img_sample_NBI, lab_sample_NBI = next(iter(train_NBI_image_label_ds))\n",
    "\n",
    "print(\"WL sample info:\")\n",
    "print(\"shape: {}, label: {} \".format(img_sample_WL.shape, lab_sample_WL))\n",
    "print(\"NBI sample info:\")\n",
    "print(\"shape: {}, label: {} \".format(img_sample_NBI.shape, lab_sample_NBI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1624550230223,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "6xlFvqqhLqjf",
    "outputId": "5a127a7d-91df-4456-da7b-ae7c875bc317"
   },
   "outputs": [],
   "source": [
    "b = train_NBI_array[0]\n",
    "plt.hist(b.ravel())\n",
    "plt.title(\"Before scaling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1624550232521,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "-a64wkX694Sn",
    "outputId": "d241f56c-387c-4d01-aad8-511f77b06f64"
   },
   "outputs": [],
   "source": [
    "a = np.array(img_sample_NBI[0])\n",
    "plt.hist(a.ravel())\n",
    "plt.title(\"After scaling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1624550236229,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "iN2I1vcy9kK5",
    "outputId": "81e63a78-9c36-4241-da4b-5910ee1b1bf8"
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title('White light')\n",
    "print(img_sample_WL[0].shape)\n",
    "plt.imshow(np.squeeze(img_sample_WL[0]) * 0.5 + 0.5, cmap='gray')\n",
    "idx = lab_sample_WL.numpy().argmax()\n",
    "plt.xlabel(idx2str[idx])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('White light with random jitter')\n",
    "plt.imshow(np.squeeze(random_jitter(img_sample_WL[0])) * 0.5 + 0.5, cmap='gray')\n",
    "idx = lab_sample_WL.numpy().argmax()\n",
    "plt.xlabel(idx2str[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 793,
     "status": "ok",
     "timestamp": 1624550238910,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "D-HKAmMS9w7o",
    "outputId": "a9eda09c-b1a4-4805-b33c-e39fd8fdcf98"
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title('NBI light')\n",
    "plt.imshow(np.squeeze(img_sample_NBI[0]) * 0.5 + 0.5, cmap='gray')\n",
    "idx = lab_sample_NBI.numpy().argmax()\n",
    "plt.xlabel(idx2str[idx])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('NBI light with random jitter')\n",
    "plt.imshow(np.squeeze(random_jitter(img_sample_NBI[0])) * 0.5 + 0.5, cmap='gray')\n",
    "idx = lab_sample_NBI.numpy().argmax()\n",
    "plt.xlabel(idx2str[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading many NBI samples (only for data understanding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = [], []\n",
    "for i in tqdm(range(25)):\n",
    "    imgs_samples, labels_samples = next(iter(train_NBI_image_label_ds.shuffle(buffer_size=len(train_NBI_imgs))))\n",
    "    images.append(imgs_samples)\n",
    "    labels.append(labels_samples)\n",
    "\n",
    "images = np.asarray(images)\n",
    "print(\"images: {}, amount of labels: {}\".format(images.shape, len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(np.squeeze(images[i])* 0.5 + 0.5)#convert (batch, high, width, #channels) into (high, width, #channels) \n",
    "    idx = labels[i].numpy().argmax()\n",
    "    plt.xlabel(\"label: {}\".format(idx2str[idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX3KegPtTQxP"
   },
   "source": [
    "# Import and reuse the Pix2Pix models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgQXeVCaTUvF"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 32068,
     "status": "ok",
     "timestamp": 1624550285613,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "eYSghJ-FTUxU",
    "outputId": "1e26baaa-0297-4397-baa1-d99617529fab"
   },
   "outputs": [],
   "source": [
    "img_sample_WL, label_sample_WL = next(iter((train_WL_image_label_ds)))\n",
    "img_sample_NBI, label_sample_NBI = next(iter((train_NBI_image_label_ds)))\n",
    "\n",
    "print(\"info de real data\")\n",
    "print(\"img shape: {}, label: {}\".format(img_sample_WL.shape, lab_sample_WL))\n",
    "print(\"min: {}, max: {}\".format(tf.reduce_min(img_sample_WL).numpy(), tf.reduce_max(img_sample_WL).numpy()))\n",
    "print(\"min: {}, max: {}\".format(tf.reduce_min(img_sample_NBI).numpy(), tf.reduce_max(img_sample_NBI).numpy()))\n",
    "\n",
    "to_NBI = generator_g([img_sample_WL])\n",
    "to_WL = generator_f([img_sample_NBI])\n",
    "\n",
    "print(\"info de fake data\")\n",
    "print(\"min: {}, max: {}\".format(tf.reduce_min(to_NBI).numpy(), tf.reduce_max(to_NBI).numpy()))\n",
    "print(\"min: {}, max: {}\".format(tf.reduce_min(to_WL).numpy(), tf.reduce_max(to_WL).numpy()))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [img_sample_WL, to_NBI, img_sample_NBI, to_WL]\n",
    "title = ['WL', 'To NBI', 'NBI', 'To WL']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    if i % 2 == 0:\n",
    "        plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
    "    else:\n",
    "        plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_sample_NBI.shape)\n",
    "print(label_sample_NBI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1624550292957,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "y536wXtvVy2d",
    "outputId": "da3776b7-3164-45d6-9345-ee26b6672e41"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Is a real NBI?')\n",
    "plt.imshow(discriminator_y([img_sample_NBI])[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Is a real WL?')\n",
    "plt.imshow(discriminator_x([img_sample_WL])[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"path/to/the/pretrained/NBI_model.h5\"\n",
    "cls_model = keras.models.load_model(model_path, compile=True)\n",
    "    \n",
    "for layer in cls_model.layers:\n",
    "    layer.trainable = False\n",
    "print(\"all freezing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0ZICF6RVlaN"
   },
   "source": [
    "## **Loss functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9v-28iXVpPu"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "class_loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjWBOfbdVpUU"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "        \n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "    return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.5 * loss\n",
    "\n",
    "def classifier_loss(y_real, y_pred):\n",
    "    return class_loss_obj(y_real, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmbs66oOWR4e"
   },
   "source": [
    "## **Initializing optimizers, generator and discriminators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_27PI0wWBEW"
   },
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5)\n",
    "\n",
    "cls_model_optimizier = tf.keras.optimizers.Adam(lr, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "har0dLBMWkQW"
   },
   "source": [
    "## **Check points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13853,
     "status": "ok",
     "timestamp": 1624550348689,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "50jrnN4jWBHF",
    "outputId": "6189da19-a93e-428b-9701-1089ade70120"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = \"../models/folders/\" + experiment\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer,\n",
    "                           cls_model=cls_model, \n",
    "                           cls_model_optimizier=cls_model_optimizier)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ave9_8XLXI4P"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    \"\"\"\n",
    "    Generates and displays side-by-side images: the input image and the corresponding predicted image\n",
    "    using the specified model.\n",
    "\n",
    "    Parameters:\n",
    "    model: A pre-trained or trained model capable of generating image predictions.\n",
    "    test_input: The input image or image batch for which the model will generate predictions.\n",
    "\n",
    "    Notes:\n",
    "    - The function assumes that the model has been trained and is capable of producing image predictions.\n",
    "    - The test_input should be in a format compatible with the model's input requirements.\n",
    "    - The function uses matplotlib to display the input image and the predicted image side by side.\n",
    "    - The input image and predicted image are plotted on the same scale, which is adjusted to lie between\n",
    "      [0, 1] to ensure proper visualization.\n",
    "    - The function does not return anything. Instead, it directly displays the images using matplotlib.\n",
    "    \"\"\"\n",
    "    # Get the prediction from the model for the test_input\n",
    "    prediction = model(test_input)\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # Create a list of images to display\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    # Plot the input image and predicted image side by side\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Scale the pixel values between [0, 1] for proper visualization\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Show the plot with both images\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PF6EGAqXeYu"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y, alpha1, alpha2, alpha3):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    real_x_img = real_x[0]\n",
    "    real_y_img = real_y[0]\n",
    "    real_y_label = real_y[1]\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y------> WL -> NBI\n",
    "        # Generator F translates Y -> X.-----> NBI -> WL\n",
    "\n",
    "        fake_y = generator_g(real_x_img, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "        #same for revert domain traslation\n",
    "        fake_x = generator_f(real_y_img, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x_img, training=True)\n",
    "        same_y = generator_g(real_y_img, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x_img, training=True)\n",
    "        disc_real_y = discriminator_y(real_y_img, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "        #image classification (adeVshyp)\n",
    "        pred_y = cls_model(fake_y)\n",
    "\n",
    "        # calculate the loss (generator)\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "        \n",
    "        # calculate the loss (classificator)\n",
    "        y_cls_loss = classifier_loss(real_y_label, pred_y)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x_img, cycled_x) + calc_cycle_loss(real_y_img, cycled_y)\n",
    "\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y_img, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x_img, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "        \n",
    "\n",
    "    ### Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                              discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss,\n",
    "                                              discriminator_y.trainable_variables)\n",
    "    #calculate the gradients for classifier network\n",
    "    cls_model_gradients = tape.gradient(y_cls_loss, cls_model.trainable_variables)\n",
    "    \n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))\n",
    "    \n",
    "    cls_model_optimizier.apply_gradients(zip(cls_model_gradients,\n",
    "                                               cls_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS807LgmXebM"
   },
   "outputs": [],
   "source": [
    "def train_and_checkpoint(ckpt_manager=None):\n",
    "    \n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        n = 0    \n",
    "\n",
    "        for image_x, image_y in tf.data.Dataset.zip((train_WL_image_label_ds, train_NBI_image_label_ds)):\n",
    "            train_step(image_x, image_y)\n",
    "            if n % 10 == 0:\n",
    "                print ('.', end='')\n",
    "            n += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        # Using a consistent image (sample_horse) so that the progress of the model\n",
    "        # is clearly visible.\n",
    "        generate_images(generator_g, img_sample_WL)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "                       \n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 19477034,
     "status": "ok",
     "timestamp": 1624569857978,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "I8J7EEadLCeq",
    "outputId": "e6e9bacb-8deb-4ffd-b7c7-adc637cfe896"
   },
   "outputs": [],
   "source": [
    "train_and_checkpoint(ckpt_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing over single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCIF4Xv9X04s"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()# **Generate using test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5626,
     "status": "ok",
     "timestamp": 1624569874801,
     "user": {
      "displayName": "Franklin Samuel Sierra Jerez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhhOZ4W8-xTi0rwGn3Yugc6h9JX2ob_3eMY8dJb=s64",
      "userId": "07062157042636671418"
     },
     "user_tz": 300
    },
    "id": "0B9kYW0eX7dO",
    "outputId": "3257a721-039b-45e0-b005-63ad5de45a19"
   },
   "outputs": [],
   "source": [
    "# frames from video path\n",
    "path =  \"../path/WL_/polyp_class/videoN/\" #inside the videoN folder must be the corresponding video frames\n",
    "# load dataset white light\n",
    "adenoma_WL = load_images(path, rgb=True)\n",
    "print(\"Adenoma WL video_1: \", adenoma_WL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adenoma_WL_array = np.asarray(adenoma_WL)\n",
    "adenoma_WL_ds = tf.data.Dataset.from_tensor_slices(adenoma_WL_array)\n",
    "adenoma_WL_ds = adenoma_WL_ds.map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "                BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in adenoma_WL_ds.take(adenoma_WL.shape[0]):\n",
    "    generate_images(generator_g, inp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPAQPpaalA3H1MyI5B0C0a1",
   "mount_file_id": "1PErs7QQlODdCoyKR28vTNul3lTGovSGc",
   "name": "cycleGan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
